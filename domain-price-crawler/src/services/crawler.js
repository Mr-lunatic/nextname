const { v4: uuidv4 } = require('uuid');
const { fetchWithRetry } = require('./nazhumi-api');
const {
  savePricingData,
  updateTaskStatus,
  logCrawlAttempt,
  getDataStats,
  getMissingCombinations,
  setConfig,
  getConfig
} = require('./database');
const { crawlLogger, logger } = require('../utils/logger');
const { CRAWL_CONFIG, PRIORITY_TLDS, PRIORITY_REGISTRARS } = require('../config/constants');

// å»¶è¿Ÿå‡½æ•°
const sleep = (ms) => new Promise(resolve => setTimeout(resolve, ms));

// ç”Ÿæˆä¼šè¯ID
const generateSessionId = () => uuidv4();

// è®°å½•é‡‡é›†ç»“æœï¼ˆä¸´æ—¶å‡½æ•°ï¼Œç›´åˆ°æ•°æ®åº“æœåŠ¡ä¸­æ·»åŠ ï¼‰
const logCrawlResult = async (sessionId, tld, registrar, status, responseTime, savedCount = 0, error = null) => {
  try {
    // ä½¿ç”¨ç°æœ‰çš„logCrawlAttemptå‡½æ•°
    await logCrawlAttempt(sessionId, tld, registrar, status, responseTime, error);
    logger.debug(`ğŸ“ Logged crawl result: ${tld}/${registrar} - ${status}`);
  } catch (err) {
    logger.warn(`âš ï¸ Failed to log crawl result: ${err.message}`);
  }
};

// æ›´æ–°ç³»ç»Ÿé…ç½®ï¼ˆä¸´æ—¶å‡½æ•°ï¼Œç›´åˆ°æ•°æ®åº“æœåŠ¡ä¸­æ·»åŠ ï¼‰
const updateSystemConfig = async (key, value) => {
  try {
    await setConfig(key, value);
  } catch (err) {
    logger.warn(`âš ï¸ Failed to update system config ${key}: ${err.message}`);
  }
};

// ä¸´æ—¶çš„savePricingDataå‡½æ•°ï¼Œä½¿ç”¨æ­£ç¡®çš„MySQLè¯­æ³•
const savePricingDataMySQL = async (pricingData) => {
  const mysql = require('mysql2/promise');

  // åˆ›å»ºæ•°æ®åº“è¿æ¥
  const connection = await mysql.createConnection({
    host: process.env.DB_HOST,
    user: process.env.DB_USER,
    password: process.env.DB_PASSWORD,
    database: process.env.DB_NAME,
    port: process.env.DB_PORT || 3306
  });

  const query = `
    INSERT INTO pricing_data (
      tld, registrar, registrar_name, registrar_url,
      registration_price, renewal_price, transfer_price,
      currency, currency_name, currency_type,
      has_promo, updated_time, crawled_at, source, is_active
    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, NOW(), 'crawler', true)
    ON DUPLICATE KEY UPDATE
      registrar_name = VALUES(registrar_name),
      registrar_url = VALUES(registrar_url),
      registration_price = VALUES(registration_price),
      renewal_price = VALUES(renewal_price),
      transfer_price = VALUES(transfer_price),
      currency = VALUES(currency),
      currency_name = VALUES(currency_name),
      currency_type = VALUES(currency_type),
      has_promo = VALUES(has_promo),
      updated_time = VALUES(updated_time),
      crawled_at = NOW()
  `;

  const values = [
    pricingData.tld,
    pricingData.registrar,
    pricingData.registrarName || pricingData.registrar,
    pricingData.registrarUrl || '',
    pricingData.registrationPrice,
    pricingData.renewalPrice,
    pricingData.transferPrice,
    pricingData.currency?.toLowerCase() || 'usd',
    pricingData.currencyName || '',
    pricingData.currencyType || '',
    pricingData.hasPromo ? 1 : 0,
    pricingData.updatedTime || new Date().toISOString()
  ];

  try {
    const [result] = await connection.execute(query, values);
    logger.debug(`âœ… Saved pricing data: ${pricingData.registrar}/${pricingData.tld}`);
    return result;
  } catch (error) {
    logger.error(`âŒ Failed to save pricing data: ${pricingData.registrar}/${pricingData.tld}`, error.message);
    throw error;
  } finally {
    await connection.end();
  }
};

// ç”Ÿæˆé‡‡é›†è®¡åˆ’
const generateCrawlPlan = async () => {
  try {
    logger.info('ğŸ“‹ Generating crawl plan...');
    
    // è·å–å½“å‰æ•°æ®ç»Ÿè®¡
    const stats = await getDataStats();
    logger.info('ğŸ“Š Current data stats:', stats);
    
    // æ ¹æ®æ•°æ®è¦†ç›–æƒ…å†µå†³å®šé‡‡é›†ç­–ç•¥
    let tlds, registrars, maxTasks;
    
    if (stats.total_records < 500) {
      // åˆå§‹é˜¶æ®µï¼šæ ¸å¿ƒTLD + ä¸»è¦æ³¨å†Œå•†
      tlds = PRIORITY_TLDS.slice(0, 8); // å‰8ä¸ªæ ¸å¿ƒTLD
      registrars = PRIORITY_REGISTRARS.slice(0, 10); // å‰10ä¸ªæ³¨å†Œå•†
      maxTasks = 80;
      logger.info('ğŸš€ Bootstrap mode: Core TLDs + Priority registrars');
    } else if (stats.total_records < 2000) {
      // æ‰©å±•é˜¶æ®µï¼šæ›´å¤šTLD
      tlds = PRIORITY_TLDS.slice(0, 20); // å‰20ä¸ªTLD
      registrars = PRIORITY_REGISTRARS.slice(0, 15); // å‰15ä¸ªæ³¨å†Œå•†
      maxTasks = 300;
      logger.info('ğŸ“ˆ Expansion mode: Extended TLDs');
    } else {
      // æ­£å¸¸è¿è¥ï¼šå®Œæ•´è¦†ç›–
      tlds = PRIORITY_TLDS; // æ‰€æœ‰ä¼˜å…ˆTLD
      registrars = PRIORITY_REGISTRARS; // æ‰€æœ‰ä¼˜å…ˆæ³¨å†Œå•†
      maxTasks = CRAWL_CONFIG.DAILY_LIMIT;
      logger.info('ğŸ¯ Normal mode: Full coverage');
    }
    
    // è·å–ç¼ºå¤±çš„ç»„åˆ
    const missingCombinations = await getMissingCombinations(tlds, registrars, 24);
    logger.info(`ğŸ•³ï¸ Found ${missingCombinations.length} missing combinations`);
    
    // é™åˆ¶ä»»åŠ¡æ•°é‡
    const tasks = missingCombinations.slice(0, maxTasks);
    
    logger.info(`ğŸ“‹ Generated crawl plan: ${tasks.length} tasks`);
    return tasks;
    
  } catch (error) {
    logger.error('Failed to generate crawl plan:', error.message);
    throw error;
  }
};

// æ‰§è¡ŒTLDé‡‡é›†ä»»åŠ¡ï¼ˆè·å–è¯¥TLDçš„æ‰€æœ‰æ³¨å†Œå•†ä»·æ ¼ï¼‰
const executeTldCrawlTask = async (sessionId, tld) => {
  const startTime = Date.now();

  try {
    logger.info(`ğŸ” Crawling TLD: ${tld}`);

    // è·å–è¯¥TLDçš„æ‰€æœ‰æ³¨å†Œå•†ä»·æ ¼
    const { getPricing } = require('./nazhumi-api');
    const allPrices = await getPricing(tld);
    const responseTime = Date.now() - startTime;

    logger.info(`âœ… Got ${allPrices.length} registrars for ${tld}`);

    // ä¿å­˜æ‰€æœ‰ä»·æ ¼æ•°æ®
    let savedCount = 0;
    for (const price of allPrices) {
      try {
        await savePricingDataMySQL(price);
        savedCount++;
      } catch (error) {
        logger.warn(`âš ï¸ Failed to save ${price.registrar}/${tld}: ${error.message}`);
      }
    }

    // è®°å½•æ—¥å¿—
    await logCrawlResult(sessionId, tld, 'all_registrars', 'completed', responseTime, savedCount);

    logger.info(`âœ… TLD crawl completed: ${tld} (${savedCount}/${allPrices.length} saved, ${responseTime}ms)`);

    return {
      tld,
      status: 'completed',
      registrarsFound: allPrices.length,
      registrarsSaved: savedCount,
      responseTime
    };

  } catch (error) {
    const responseTime = Date.now() - startTime;

    // è®°å½•å¤±è´¥æ—¥å¿—
    await logCrawlResult(sessionId, tld, 'all_registrars', 'failed', responseTime, 0, error.message);

    logger.error(`âŒ TLD crawl failed: ${tld} - ${error.message} (${responseTime}ms)`);

    return {
      tld,
      status: 'failed',
      error: error.message,
      responseTime
    };
  }
};

// æ‰§è¡Œå•ä¸ªé‡‡é›†ä»»åŠ¡ï¼ˆå‘åå…¼å®¹ï¼‰
const executeCrawlTask = async (sessionId, task) => {
  const { tld, registrar } = task;
  const startTime = Date.now();

  try {
    // è·å–ä»·æ ¼æ•°æ®
    const pricingData = await fetchWithRetry(registrar, tld);
    const responseTime = Date.now() - startTime;

    // ä¿å­˜åˆ°æ•°æ®åº“
    await savePricingData(pricingData);

    // æ›´æ–°ä»»åŠ¡çŠ¶æ€
    await updateTaskStatus(tld, registrar, 'completed');

    // è®°å½•æ—¥å¿—
    await logCrawlAttempt(sessionId, tld, registrar, 'success', responseTime);
    crawlLogger.success(sessionId, tld, registrar, responseTime);
    
    return {
      status: 'success',
      tld,
      registrar,
      responseTime,
      data: pricingData
    };
    
  } catch (error) {
    const responseTime = Date.now() - startTime;
    
    // æ›´æ–°ä»»åŠ¡çŠ¶æ€
    await updateTaskStatus(tld, registrar, 'failed', error.message);
    
    // è®°å½•æ—¥å¿—
    await logCrawlAttempt(sessionId, tld, registrar, 'failed', responseTime, error.message);
    crawlLogger.error(sessionId, tld, registrar, error, responseTime);
    
    return {
      status: 'failed',
      tld,
      registrar,
      responseTime,
      error: error.message
    };
  }
};

// ä¸»é‡‡é›†å‡½æ•°
const executeCrawl = async (options = {}) => {
  const sessionId = uuidv4();
  const {
    maxTasks = null,
    interval = CRAWL_CONFIG.REQUEST_INTERVAL,
    onProgress = null
  } = options;
  
  try {
    // è®°å½•é‡‡é›†å¼€å§‹
    await setConfig('crawl_session_id', sessionId);
    await setConfig('last_crawl_start', new Date().toISOString());
    
    // ç”Ÿæˆé‡‡é›†è®¡åˆ’
    const tasks = await generateCrawlPlan();
    
    if (tasks.length === 0) {
      logger.info('ğŸ‰ No crawling needed - all data is fresh!');
      return {
        sessionId,
        totalTasks: 0,
        completed: 0,
        failed: 0,
        skipped: 0,
        duration: 0
      };
    }
    
    // é™åˆ¶ä»»åŠ¡æ•°é‡
    const finalTasks = maxTasks ? tasks.slice(0, maxTasks) : tasks;
    
    crawlLogger.start(sessionId, finalTasks.length);
    
    const results = {
      sessionId,
      totalTasks: finalTasks.length,
      completed: 0,
      failed: 0,
      skipped: 0,
      startTime: Date.now(),
      errors: []
    };
    
    // æ‰§è¡Œé‡‡é›†ä»»åŠ¡
    for (let i = 0; i < finalTasks.length; i++) {
      const task = finalTasks[i];
      
      try {
        // å¸¦å®½å‹å¥½çš„é—´éš”
        if (i > 0) {
          await sleep(interval);
        }
        
        // æ‰§è¡Œä»»åŠ¡
        const result = await executeCrawlTask(sessionId, task);
        
        if (result.status === 'success') {
          results.completed++;
        } else {
          results.failed++;
          results.errors.push({
            tld: task.tld,
            registrar: task.registrar,
            error: result.error
          });
        }
        
        // è¿›åº¦å›è°ƒ
        if (onProgress) {
          onProgress(i + 1, finalTasks.length, task, result);
        }
        
        // å®šæœŸè¿›åº¦æŠ¥å‘Š
        if ((i + 1) % 50 === 0) {
          crawlLogger.progress(sessionId, i + 1, finalTasks.length, task);
        }
        
        // æ£€æŸ¥å¤±è´¥ç‡
        const failureRate = results.failed / (results.completed + results.failed);
        if (failureRate > CRAWL_CONFIG.FAILURE_RATE_THRESHOLD && (results.completed + results.failed) > 20) {
          logger.warn(`âš ï¸ High failure rate detected: ${(failureRate * 100).toFixed(1)}%. Stopping crawl.`);
          break;
        }
        
      } catch (error) {
        logger.error(`âŒ Unexpected error processing task ${task.tld}/${task.registrar}:`, error.message);
        results.failed++;
      }
    }
    
    // è®¡ç®—æ€»è€—æ—¶
    results.duration = Date.now() - results.startTime;
    results.successRate = ((results.completed / results.totalTasks) * 100).toFixed(1);
    
    // è®°å½•é‡‡é›†å®Œæˆ
    await setConfig('last_crawl_end', new Date().toISOString());
    await setConfig('last_crawl_stats', JSON.stringify({
      completed: results.completed,
      failed: results.failed,
      successRate: results.successRate,
      duration: results.duration
    }));
    
    crawlLogger.complete(sessionId, results);
    
    return results;
    
  } catch (error) {
    logger.error('âŒ Crawl execution failed:', error.message);
    throw error;
  }
};

// æ£€æŸ¥æ˜¯å¦åœ¨é‡‡é›†æ—¶é—´çª—å£å†…
const isInCrawlWindow = () => {
  const now = new Date();
  const hour = now.getHours();
  
  return hour >= CRAWL_CONFIG.CRAWL_WINDOW.START_HOUR && 
         hour < CRAWL_CONFIG.CRAWL_WINDOW.END_HOUR;
};

// è·å–é‡‡é›†çŠ¶æ€
const getCrawlStatus = async () => {
  try {
    const stats = await getDataStats();
    const lastCrawlStart = await getConfig('last_crawl_start');
    const lastCrawlEnd = await getConfig('last_crawl_end');
    const lastCrawlStats = await getConfig('last_crawl_stats');
    const currentSessionId = await getConfig('crawl_session_id');
    
    return {
      dataStats: stats,
      lastCrawlStart,
      lastCrawlEnd,
      lastCrawlStats: lastCrawlStats ? JSON.parse(lastCrawlStats) : null,
      currentSessionId,
      isInCrawlWindow: isInCrawlWindow(),
      nextCrawlWindow: getNextCrawlWindow()
    };
  } catch (error) {
    logger.error('Failed to get crawl status:', error.message);
    throw error;
  }
};

// è·å–ä¸‹æ¬¡é‡‡é›†çª—å£æ—¶é—´
const getNextCrawlWindow = () => {
  const now = new Date();
  const tomorrow = new Date(now);
  tomorrow.setDate(tomorrow.getDate() + 1);
  tomorrow.setHours(CRAWL_CONFIG.CRAWL_WINDOW.START_HOUR, 0, 0, 0);
  
  return tomorrow.toISOString();
};

// æ–°çš„TLDé‡‡é›†å‡½æ•°
const crawlByTlds = async () => {
  const sessionId = generateSessionId();
  const startTime = Date.now();

  try {
    logger.info('ğŸš€ Starting TLD-based crawl session:', sessionId);

    // è·å–è¦é‡‡é›†çš„TLDåˆ—è¡¨
    const { PRIORITY_TLDS } = require('../config/constants');
    const tlds = PRIORITY_TLDS.slice(0, 5); // é™åˆ¶æ•°é‡ï¼Œé¿å…è¿‡åº¦ä½¿ç”¨API

    logger.info(`ğŸ“‹ Crawling ${tlds.length} TLDs: ${tlds.join(', ')}`);

    const results = [];
    let totalRegistrars = 0;
    let totalSaved = 0;

    // é€ä¸ªé‡‡é›†TLD
    for (let i = 0; i < tlds.length; i++) {
      const tld = tlds[i];

      try {
        // æ·»åŠ å»¶è¿Ÿï¼Œé¿å…APIé¢‘ç‡é™åˆ¶
        if (i > 0) {
          logger.info(`â³ Waiting 5 seconds before next TLD...`);
          await new Promise(resolve => setTimeout(resolve, 5000));
        }

        const result = await executeTldCrawlTask(sessionId, tld);
        results.push(result);

        if (result.status === 'completed') {
          totalRegistrars += result.registrarsFound;
          totalSaved += result.registrarsSaved;
        }

        logger.info(`ğŸ“Š Progress: ${i + 1}/${tlds.length} TLDs completed`);

      } catch (error) {
        logger.error(`âŒ Failed to crawl TLD ${tld}:`, error.message);
        results.push({
          tld,
          status: 'failed',
          error: error.message
        });
      }
    }

    const duration = Date.now() - startTime;
    const successCount = results.filter(r => r.status === 'completed').length;
    const failureCount = results.filter(r => r.status === 'failed').length;

    // æ›´æ–°ç³»ç»Ÿé…ç½®
    await updateSystemConfig('last_crawl_time', new Date().toISOString());
    await updateSystemConfig('last_crawl_results', JSON.stringify({
      sessionId,
      totalTlds: tlds.length,
      completed: successCount,
      failed: failureCount,
      totalRegistrars,
      totalSaved,
      duration: Math.round(duration / 1000),
      timestamp: new Date().toISOString()
    }));

    logger.info(`ğŸ‰ TLD crawl session completed: ${successCount}/${tlds.length} TLDs, ${totalSaved} records saved (${Math.round(duration / 1000)}s)`);

    return {
      sessionId,
      status: 'completed',
      tlds: tlds.length,
      completed: successCount,
      failed: failureCount,
      totalRegistrars,
      totalSaved,
      duration: Math.round(duration / 1000)
    };

  } catch (error) {
    const duration = Date.now() - startTime;
    logger.error('âŒ TLD crawl session failed:', error.message);

    return {
      sessionId,
      status: 'failed',
      error: error.message,
      duration: Math.round(duration / 1000)
    };
  }
};

// æµ‹è¯•é‡‡é›†åŠŸèƒ½
const testCrawl = async () => {
  logger.info('ğŸ§ª Testing crawl functionality...');

  try {
    // ä½¿ç”¨æ–°çš„TLDé‡‡é›†æ–¹æ³•æµ‹è¯•
    const result = await executeTldCrawlTask('test-session', 'com');

    logger.info('âœ… Crawl test completed:', result);
    return result;

  } catch (error) {
    logger.error('âŒ Crawl test failed:', error.message);
    throw error;
  }
};

module.exports = {
  executeCrawl,
  generateCrawlPlan,
  executeCrawlTask,
  executeTldCrawlTask,
  crawlByTlds,
  isInCrawlWindow,
  getCrawlStatus,
  testCrawl
};
