#!/usr/bin/env python3
import requests
import json
import time
import mysql.connector
from bs4 import BeautifulSoup
import re
from datetime import datetime
import os
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class NazhumiSpider:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })
        
        self.base_url = 'https://www.nazhumi.com'
        self.all_data = []
        self.support_matrix = {}
        self.tld_list = []
        self.registrar_list = set()
        self.failed_tlds = []
        
    def get_all_tlds(self):
        """ä½¿ç”¨å·²çŸ¥çš„å¸¸è§TLDåˆ—è¡¨ï¼Œæˆ–è€…ä»å…¶ä»–æ¥æºè·å–"""
        try:
            print("ğŸ” Using predefined TLD list...")

            # ä½¿ç”¨å¸¸è§çš„TLDåˆ—è¡¨ï¼ˆåŸºäºæˆ‘ä»¬ä¹‹å‰çš„æµ‹è¯•ç»“æœï¼‰
            common_tlds = [
                # é€šç”¨é¡¶çº§åŸŸå
                'com', 'net', 'org', 'info', 'biz', 'name', 'pro', 'mobi', 'travel', 'jobs', 'tel', 'asia', 'xxx', 'cat', 'coop', 'museum', 'aero', 'int', 'post',

                # æ–°é€šç”¨é¡¶çº§åŸŸå
                'xyz', 'top', 'online', 'site', 'tech', 'store', 'blog', 'news', 'cloud', 'space', 'website', 'live', 'studio', 'design', 'art', 'shop', 'digital', 'email', 'host', 'domains',

                # å›½å®¶ä»£ç é¡¶çº§åŸŸå
                'cn', 'com.cn', 'net.cn', 'org.cn', 'gov.cn', 'uk', 'co.uk', 'org.uk', 'me.uk', 'de', 'fr', 'it', 'es', 'nl', 'ca', 'au', 'jp', 'kr', 'in', 'br', 'mx', 'ru', 'pl', 'se', 'no', 'dk', 'fi', 'be', 'ch', 'at', 'ie', 'pt', 'gr', 'cz', 'hk', 'tw', 'sg', 'my', 'th', 'ph', 'id', 'vn', 'ae', 'sa',

                # å…¶ä»–TLD
                'io', 'co', 'ai', 'app', 'dev', 'me', 'tv', 'cc', 'ws'
            ]

            # éªŒè¯TLDæ˜¯å¦çœŸçš„å­˜åœ¨
            verified_tlds = []
            print(f"ğŸ“‹ Verifying {len(common_tlds)} TLDs...")

            for i, tld in enumerate(common_tlds):
                try:
                    # å¿«é€Ÿæ£€æŸ¥TLDé¡µé¢æ˜¯å¦å­˜åœ¨
                    url = f'{self.base_url}/domain/{tld}'
                    response = self.session.head(url, timeout=10)

                    if response.status_code == 200:
                        verified_tlds.append(tld)
                        print(f"âœ… {tld} - verified")
                    else:
                        print(f"âŒ {tld} - not found")

                except Exception as e:
                    print(f"âš ï¸ {tld} - error: {e}")

                # è¿›åº¦æ˜¾ç¤º
                if (i + 1) % 10 == 0:
                    print(f"ğŸ“Š Progress: {i + 1}/{len(common_tlds)} checked, {len(verified_tlds)} verified")

                # å‹å¥½å»¶è¿Ÿ
                time.sleep(0.5)

            self.tld_list = verified_tlds
            print(f"ğŸ‰ Total verified TLDs: {len(verified_tlds)}")

            # ä¿å­˜TLDåˆ—è¡¨
            os.makedirs('data', exist_ok=True)
            with open('data/nazhumi_tlds.json', 'w', encoding='utf-8') as f:
                json.dump(verified_tlds, f, indent=2, ensure_ascii=False)

            return verified_tlds

        except Exception as e:
            print(f"âŒ Error getting TLD list: {e}")
            return []
    
    def get_tld_registrars(self, tld):
        """ä» https://www.nazhumi.com/domain/{tld}/1 è·å–ç‰¹å®šTLDçš„æ‰€æœ‰æ³¨å†Œå•†å’Œä»·æ ¼"""
        try:
            print(f"ğŸ” Getting registrars for .{tld}...")
            
            page = 1
            tld_data = []
            
            while True:
                url = f'{self.base_url}/domain/{tld}/{page}'
                
                response = self.session.get(url)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æŸ¥æ‰¾ä»·æ ¼è¡¨æ ¼æˆ–åˆ—è¡¨
                price_rows = self.parse_price_table(soup, tld)
                
                if not price_rows:
                    if page == 1:
                        print(f"âš ï¸ No price data found for .{tld}")
                        self.failed_tlds.append(tld)
                    break
                
                tld_data.extend(price_rows)
                print(f"âœ… Found {len(price_rows)} registrars on page {page} for .{tld}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
                next_page = soup.find('a', text=re.compile(r'ä¸‹ä¸€é¡µ|Next|>'))
                if not next_page:
                    break
                
                page += 1
                time.sleep(1)  # å‹å¥½å»¶è¿Ÿ
            
            # æ›´æ–°æ”¯æŒçŸ©é˜µ
            for data in tld_data:
                registrar = data['registrar']
                self.registrar_list.add(registrar)
                
                if registrar not in self.support_matrix:
                    self.support_matrix[registrar] = []
                if tld not in self.support_matrix[registrar]:
                    self.support_matrix[registrar].append(tld)
            
            print(f"ğŸ‰ Total registrars for .{tld}: {len(tld_data)}")
            return tld_data
            
        except Exception as e:
            print(f"âŒ Error getting registrars for .{tld}: {e}")
            self.failed_tlds.append(tld)
            return []
    
    def parse_price_table(self, soup, tld):
        """è§£æä»·æ ¼è¡¨æ ¼ï¼Œæå–æ³¨å†Œå•†å’Œä»·æ ¼ä¿¡æ¯"""
        try:
            price_data = []

            # æŸ¥æ‰¾ä»·æ ¼è¡¨æ ¼ - åŸºäºå®é™…HTMLç»“æ„
            table = soup.find('table', class_='table')
            if not table:
                print(f"âš ï¸ No price table found for .{tld}")
                return []

            # æŸ¥æ‰¾è¡¨æ ¼è¡Œ
            tbody = table.find('tbody')
            if not tbody:
                print(f"âš ï¸ No tbody found in table for .{tld}")
                return []

            rows = tbody.find_all('tr')
            print(f"ğŸ“‹ Found {len(rows)} price rows for .{tld}")

            for row in rows:
                cells = row.find_all('td')
                if len(cells) >= 5:  # æ³¨å†Œå•†ã€æ³¨å†Œä»·æ ¼ã€ç»­è´¹ä»·æ ¼ã€è½¬å…¥ä»·æ ¼ã€æ“ä½œ
                    data = self.extract_price_data(cells, tld)
                    if data:
                        price_data.append(data)

            return price_data

        except Exception as e:
            print(f"âš ï¸ Error parsing price table for .{tld}: {e}")
            return []
    
    def extract_price_data(self, cells, tld):
        """ä»è¡¨æ ¼è¡Œä¸­æå–ä»·æ ¼æ•°æ® - åŸºäºå®é™…HTMLç»“æ„"""
        try:
            if len(cells) < 5:  # æ³¨å†Œå•†ã€æ³¨å†Œä»·æ ¼ã€ç»­è´¹ä»·æ ¼ã€è½¬å…¥ä»·æ ¼ã€æ“ä½œ
                return None

            # æå–æ³¨å†Œå•†ä¿¡æ¯ (ç¬¬1åˆ—)
            registrar_cell = cells[0]
            registrar_link = registrar_cell.find('a')

            if not registrar_link:
                return None

            registrar_name = registrar_link.get_text(strip=True)
            href = registrar_link.get('href', '')

            # ä»é“¾æ¥ä¸­æå–æ³¨å†Œå•†ä»£ç : /registrar/spaceship -> spaceship
            registrar_match = re.search(r'/registrar/([^/]+)', href)
            if registrar_match:
                registrar = registrar_match.group(1)
            else:
                registrar = registrar_name.lower().replace(' ', '').replace('.', '')

            # æå–å®˜ç½‘é“¾æ¥ (ç¬¬5åˆ—)
            registrar_url = ''
            if len(cells) > 4:
                url_link = cells[4].find('a')
                if url_link:
                    registrar_url = url_link.get('href', '')

            # æå–ä»·æ ¼ä¿¡æ¯
            reg_price_text = cells[1].get_text(strip=True)      # æ³¨å†Œä»·æ ¼
            renew_price_text = cells[2].get_text(strip=True)    # ç»­è´¹ä»·æ ¼
            transfer_price_text = cells[3].get_text(strip=True) # è½¬å…¥ä»·æ ¼

            # è§£æä»·æ ¼å’Œè´§å¸
            reg_price, reg_currency = self.parse_price_and_currency(reg_price_text)
            renew_price, renew_currency = self.parse_price_and_currency(renew_price_text)
            transfer_price, transfer_currency = self.parse_price_and_currency(transfer_price_text)

            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæœ‰æ•ˆè´§å¸ä½œä¸ºä¸»è´§å¸
            currency = reg_currency or renew_currency or transfer_currency or 'usd'

            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆä»·æ ¼
            if not any([reg_price, renew_price, transfer_price]):
                return None

            data = {
                'tld': tld,
                'registrar': registrar,
                'registrar_name': registrar_name,
                'registrar_url': registrar_url,
                'registration_price': reg_price,
                'renewal_price': renew_price,
                'transfer_price': transfer_price,
                'currency': currency.lower(),
                'currency_name': self.get_currency_name(currency),
                'currency_type': self.get_currency_type(currency),
                'has_promo': False,
                'updated_time': datetime.now().isoformat(),
                'crawled_at': datetime.now().isoformat(),
                'source': 'nazhumi_web'
            }

            return data

        except Exception as e:
            print(f"âš ï¸ Error extracting price data: {e}")
            return None
    
    def extract_price_from_div(self, div, tld):
        """ä»divå…ƒç´ ä¸­æå–ä»·æ ¼æ•°æ®"""
        try:
            text = div.get_text()
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…HTMLç»“æ„æ¥è§£æ
            # æš‚æ—¶è¿”å›Noneï¼Œç­‰çœ‹åˆ°å®é™…é¡µé¢ç»“æ„åå†å®ç°
            return None
        except:
            return None
    
    def parse_price_and_currency(self, price_text):
        """è§£æä»·æ ¼æ–‡æœ¬ï¼Œè¿”å›ä»·æ ¼å’Œè´§å¸ - åŸºäºå®é™…æ ¼å¼"""
        try:
            if not price_text or price_text.strip() in ['--', '-', 'N/A', '', 'æš‚æ— ']:
                return None, None

            # ç§»é™¤ç©ºæ ¼å’Œæ¢è¡Œ
            price_text = price_text.strip().replace('\n', '').replace('\r', '').replace(',', '')

            # æ£€æµ‹è´§å¸ç±»å‹å’Œæå–ä»·æ ¼
            currency = 'usd'  # é»˜è®¤
            price = None

            # ç¾å…ƒæ ¼å¼: "5.62 ç¾å…ƒ" æˆ– "$5.62"
            if 'ç¾å…ƒ' in price_text or '$' in price_text:
                currency = 'usd'
                price_match = re.search(r'(\d+(?:\.\d+)?)', price_text)
                if price_match:
                    price = float(price_match.group(1))

            # äººæ°‘å¸æ ¼å¼: "69 å…ƒ" æˆ– "Â¥69"
            elif 'å…ƒ' in price_text or 'Â¥' in price_text or 'CNY' in price_text.upper():
                currency = 'cny'
                price_match = re.search(r'(\d+(?:\.\d+)?)', price_text)
                if price_match:
                    price = float(price_match.group(1))

            # æ¬§å…ƒæ ¼å¼: "8 æ¬§å…ƒ" æˆ– "â‚¬8"
            elif 'æ¬§å…ƒ' in price_text or 'â‚¬' in price_text or 'EUR' in price_text.upper():
                currency = 'eur'
                price_match = re.search(r'(\d+(?:\.\d+)?)', price_text)
                if price_match:
                    price = float(price_match.group(1))

            # è‹±é•‘æ ¼å¼
            elif 'è‹±é•‘' in price_text or 'Â£' in price_text or 'GBP' in price_text.upper():
                currency = 'gbp'
                price_match = re.search(r'(\d+(?:\.\d+)?)', price_text)
                if price_match:
                    price = float(price_match.group(1))

            # é»˜è®¤å¤„ç†ï¼šåªæœ‰æ•°å­—
            else:
                price_match = re.search(r'(\d+(?:\.\d+)?)', price_text)
                if price_match:
                    price = float(price_match.group(1))
                    currency = 'usd'  # é»˜è®¤ç¾å…ƒ

            return price, currency

        except Exception as e:
            print(f"âš ï¸ Error parsing price '{price_text}': {e}")
            return None, None
    
    def get_currency_name(self, currency):
        """è·å–è´§å¸åç§°"""
        currency_names = {
            'usd': 'US Dollar',
            'cny': 'äººæ°‘å¸',
            'eur': 'æ¬§å…ƒ',
            'gbp': 'è‹±é•‘',
            'jpy': 'æ—¥å…ƒ'
        }
        return currency_names.get(currency.lower(), currency.upper())
    
    def get_currency_type(self, currency):
        """è·å–è´§å¸ç±»å‹"""
        currency_types = {
            'usd': 'ç¾å…ƒ',
            'cny': 'äººæ°‘å¸',
            'eur': 'æ¬§å…ƒ',
            'gbp': 'è‹±é•‘',
            'jpy': 'æ—¥å…ƒ'
        }
        return currency_types.get(currency.lower(), currency.upper())

    def save_to_mysql(self, data_list):
        """ä¿å­˜æ•°æ®åˆ°MySQL"""
        if not data_list:
            return 0

        try:
            connection = mysql.connector.connect(
                host=os.getenv('DB_HOST'),
                user=os.getenv('DB_USER'),
                password=os.getenv('DB_PASSWORD'),
                database=os.getenv('DB_NAME'),
                port=int(os.getenv('DB_PORT', 3306))
            )

            cursor = connection.cursor()

            insert_query = """
            INSERT INTO pricing_data (
                tld, registrar, registrar_name, registrar_url,
                registration_price, renewal_price, transfer_price,
                currency, currency_name, currency_type,
                has_promo, updated_time, crawled_at, source, is_active
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            ON DUPLICATE KEY UPDATE
                registrar_name = VALUES(registrar_name),
                registrar_url = VALUES(registrar_url),
                registration_price = VALUES(registration_price),
                renewal_price = VALUES(renewal_price),
                transfer_price = VALUES(transfer_price),
                currency = VALUES(currency),
                currency_name = VALUES(currency_name),
                currency_type = VALUES(currency_type),
                has_promo = VALUES(has_promo),
                updated_time = VALUES(updated_time),
                crawled_at = VALUES(crawled_at)
            """

            saved_count = 0
            for data in data_list:
                try:
                    values = (
                        data['tld'],
                        data['registrar'],
                        data['registrar_name'],
                        data['registrar_url'],
                        data['registration_price'],
                        data['renewal_price'],
                        data['transfer_price'],
                        data['currency'],
                        data['currency_name'],
                        data['currency_type'],
                        data['has_promo'],
                        data['updated_time'],
                        data['crawled_at'],
                        data['source'],
                        True  # is_active
                    )

                    cursor.execute(insert_query, values)
                    saved_count += 1

                except Exception as e:
                    print(f"âš ï¸ Error saving {data['registrar']}/{data['tld']}: {e}")

            connection.commit()
            print(f"âœ… Saved {saved_count}/{len(data_list)} records to MySQL")
            return saved_count

        except Exception as e:
            print(f"âŒ MySQL error: {e}")
            return 0
        finally:
            if 'connection' in locals() and connection.is_connected():
                cursor.close()
                connection.close()

    def save_progress(self):
        """ä¿å­˜è¿›åº¦å’Œç»Ÿè®¡ä¿¡æ¯"""
        try:
            os.makedirs('data', exist_ok=True)

            # ä¿å­˜æ”¯æŒçŸ©é˜µ
            with open('data/nazhumi_support_matrix.json', 'w', encoding='utf-8') as f:
                json.dump(self.support_matrix, f, indent=2, ensure_ascii=False)

            # ä¿å­˜æ³¨å†Œå•†åˆ—è¡¨
            with open('data/nazhumi_registrars.json', 'w', encoding='utf-8') as f:
                json.dump(list(self.registrar_list), f, indent=2, ensure_ascii=False)

            # ä¿å­˜å¤±è´¥çš„TLDåˆ—è¡¨
            with open('data/nazhumi_failed_tlds.json', 'w', encoding='utf-8') as f:
                json.dump(self.failed_tlds, f, indent=2, ensure_ascii=False)

            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            stats = {
                'crawl_time': datetime.now().isoformat(),
                'total_tlds': len(self.tld_list),
                'successful_tlds': len(self.tld_list) - len(self.failed_tlds),
                'failed_tlds': len(self.failed_tlds),
                'total_registrars': len(self.registrar_list),
                'total_combinations': sum(len(tlds) for tlds in self.support_matrix.values()),
                'success_rate': f"{((len(self.tld_list) - len(self.failed_tlds)) / len(self.tld_list) * 100):.1f}%" if self.tld_list else "0%",
                'registrar_stats': {
                    registrar: {
                        'supported_tlds': len(tlds),
                        'tld_list': tlds
                    }
                    for registrar, tlds in self.support_matrix.items()
                },
                'failed_tld_list': self.failed_tlds
            }

            with open('data/nazhumi_crawl_stats.json', 'w', encoding='utf-8') as f:
                json.dump(stats, f, indent=2, ensure_ascii=False)

            print("âœ… Progress and stats saved to data/ directory")

        except Exception as e:
            print(f"âŒ Error saving progress: {e}")

    def run_full_crawl(self):
        """è¿è¡Œå®Œæ•´çš„nazhumiç½‘ç«™çˆ¬å–"""
        print("ğŸš€ Starting full nazhumi website crawl...")
        print("ğŸ“‹ Phase 1: Getting all supported TLDs")

        # é˜¶æ®µ1: è·å–æ‰€æœ‰TLD
        tld_list = self.get_all_tlds()
        if not tld_list:
            print("âŒ No TLDs found, exiting...")
            return

        print(f"\nğŸ“‹ Phase 2: Crawling {len(tld_list)} TLDs for registrar data")

        # é˜¶æ®µ2: çˆ¬å–æ¯ä¸ªTLDçš„æ³¨å†Œå•†æ•°æ®
        all_data = []
        for i, tld in enumerate(tld_list):
            print(f"\nğŸ“Š Progress: {i+1}/{len(tld_list)} - Crawling .{tld}")

            tld_data = self.get_tld_registrars(tld)
            if tld_data:
                all_data.extend(tld_data)

                # æ¯ä¸ªTLDå¤„ç†å®Œåç«‹å³ä¿å­˜åˆ°æ•°æ®åº“
                saved_count = self.save_to_mysql(tld_data)
                print(f"ğŸ’¾ Saved {saved_count} records for .{tld}")

            # æ¯10ä¸ªTLDä¿å­˜ä¸€æ¬¡è¿›åº¦
            if (i + 1) % 10 == 0:
                self.save_progress()
                print(f"\nğŸ“ˆ Progress saved: {len(all_data)} total records so far")

            # å‹å¥½å»¶è¿Ÿ
            if i < len(tld_list) - 1:
                time.sleep(2)  # 2ç§’å»¶è¿Ÿ

        # æœ€ç»ˆä¿å­˜
        self.save_progress()

        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        print(f"\nğŸ‰ Full crawl completed!")
        print(f"ğŸ“Š Total TLDs processed: {len(tld_list)}")
        print(f"ğŸ“Š Successful TLDs: {len(tld_list) - len(self.failed_tlds)}")
        print(f"ğŸ“Š Failed TLDs: {len(self.failed_tlds)}")
        print(f"ğŸ“Š Total registrars found: {len(self.registrar_list)}")
        print(f"ğŸ“Š Total price records: {len(all_data)}")
        print(f"ğŸ“Š Average registrars per TLD: {len(all_data) / (len(tld_list) - len(self.failed_tlds)):.1f}" if len(tld_list) > len(self.failed_tlds) else "N/A")

        if self.failed_tlds:
            print(f"\nâš ï¸ Failed TLDs: {', '.join(self.failed_tlds[:10])}" + ("..." if len(self.failed_tlds) > 10 else ""))

if __name__ == "__main__":
    spider = NazhumiSpider()
    spider.run_full_crawl()
